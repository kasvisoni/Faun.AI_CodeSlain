{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13a4cd72",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m detection \u001b[38;5;129;01min\u001b[39;00m out:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m detection:\n\u001b[1;32m---> 41\u001b[0m         scores \u001b[38;5;241m=\u001b[39m obj[\u001b[38;5;241m5\u001b[39m:]\n\u001b[0;32m     42\u001b[0m         class_id \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(scores)\n\u001b[0;32m     43\u001b[0m         confidence \u001b[38;5;241m=\u001b[39m scores[class_id]\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "def classify_animal_simulated(cropped_image):\n",
    "    # Simulate classification (replace with actual model or logic)\n",
    "    # Example: randomly return 'Healthy', 'Injured', or 'Dead'\n",
    "    return np.random.choice(['Healthy', 'Injured', 'Dead'])\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            for obj in detection:\n",
    "                scores = obj[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:  # Confidence threshold\n",
    "                    center_x = int(obj[0] * width)\n",
    "                    center_y = int(obj[1] * height)\n",
    "                    w = int(obj[2] * width)\n",
    "                    h = int(obj[3] * height)\n",
    "\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Draw the bounding boxes on the frame\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            cropped_img = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            # Simulate classification\n",
    "            state = classify_animal_simulated(cropped_img)\n",
    "\n",
    "            # Determine color based on simulated state\n",
    "            if state == \"Dead\":\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            elif state == \"Injured\":\n",
    "                color = (0, 255, 255)  # Yellow for injured\n",
    "                alert_message = \"Alert: Animal Injured\"\n",
    "            else:\n",
    "                color = (0, 255, 0)  # Green for healthy\n",
    "                alert_message = \"Animal Healthy\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{label} ({state})\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            \n",
    "            # Display alert message (or handle it as needed)\n",
    "            print(alert_message)  # Example alert system; replace with actual notification system if needed\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507df713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco.names file downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://github.com/pjreddie/darknet/raw/master/data/coco.names\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"coco.names\", \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "print(\"coco.names file downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a8c1aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco.names file exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.isfile(\"coco.names\"):\n",
    "    print(\"coco.names file exists.\")\n",
    "else:\n",
    "    print(\"coco.names file does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185a19d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m net \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov3.weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov3.cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mgetLayerNames()\n\u001b[1;32m----> 7\u001b[0m output_layers \u001b[38;5;241m=\u001b[39m [layer_names[i[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mgetUnconnectedOutLayers()]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load COCO labels\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco.names\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m net \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov3.weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov3.cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mgetLayerNames()\n\u001b[1;32m----> 7\u001b[0m output_layers \u001b[38;5;241m=\u001b[39m [layer_names[i[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mgetUnconnectedOutLayers()]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load COCO labels\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco.names\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "if not video_capture.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "    exit()\n",
    "\n",
    "def classify_animal_simulated(cropped_image):\n",
    "    # Simulate classification (replace with actual model or logic)\n",
    "    return np.random.choice(['Healthy', 'Injured', 'Dead'])\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"End of video or error reading frame.\")\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            for obj in detection:\n",
    "                scores = obj[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5:  # Confidence threshold\n",
    "                    center_x = int(obj[0] * width)\n",
    "                    center_y = int(obj[1] * height)\n",
    "                    w = int(obj[2] * width)\n",
    "                    h = int(obj[3] * height)\n",
    "\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "    # Perform non-max suppression to remove overlapping boxes\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    if indexes is not None:\n",
    "        indexes = indexes.flatten()  # Flatten the indexes array\n",
    "\n",
    "        # Draw the bounding boxes on the frame\n",
    "        for i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            cropped_img = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            # Simulate classification\n",
    "            state = classify_animal_simulated(cropped_img)\n",
    "\n",
    "            # Determine color based on simulated state\n",
    "            if state == \"Dead\":\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            elif state == \"Injured\":\n",
    "                color = (0, 255, 255)  # Yellow for injured\n",
    "                alert_message = \"Alert: Animal Injured\"\n",
    "            else:\n",
    "                color = (0, 255, 0)  # Green for healthy\n",
    "                alert_message = \"Animal Healthy\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{label} ({state})\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            \n",
    "            # Display alert message (or handle it as needed)\n",
    "            print(alert_message)\n",
    "    else:\n",
    "        print(\"No valid indexes returned by NMS.\")\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97c720af",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m net \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov3.weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov3.cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mgetLayerNames()\n\u001b[1;32m----> 7\u001b[0m output_layers \u001b[38;5;241m=\u001b[39m [layer_names[i[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mgetUnconnectedOutLayers()]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load COCO labels\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco.names\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m net \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov3.weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov3.cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mgetLayerNames()\n\u001b[1;32m----> 7\u001b[0m output_layers \u001b[38;5;241m=\u001b[39m [layer_names[i[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mgetUnconnectedOutLayers()]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load COCO labels\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco.names\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "if not video_capture.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "    exit()\n",
    "\n",
    "def classify_animal_simulated(cropped_image):\n",
    "    # Simulate classification (replace with actual model or logic)\n",
    "    return np.random.choice(['Healthy', 'Injured', 'Dead'])\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"End of video or error reading frame.\")\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            for obj in detection:\n",
    "                scores = obj[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5:  # Confidence threshold\n",
    "                    center_x = int(obj[0] * width)\n",
    "                    center_y = int(obj[1] * height)\n",
    "                    w = int(obj[2] * width)\n",
    "                    h = int(obj[3] * height)\n",
    "\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "    # Perform non-max suppression to remove overlapping boxes\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    if indexes is not None:\n",
    "        indexes = indexes.flatten()  # Flatten the indexes array\n",
    "\n",
    "        # Draw the bounding boxes on the frame\n",
    "        for i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            cropped_img = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            # Simulate classification\n",
    "            state = classify_animal_simulated(cropped_img)\n",
    "\n",
    "            # Determine color based on simulated state\n",
    "            if state == \"Dead\":\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            elif state == \"Injured\":\n",
    "                color = (0, 255, 255)  # Yellow for injured\n",
    "                alert_message = \"Alert: Animal Injured\"\n",
    "            else:\n",
    "                color = (0, 255, 0)  # Green for healthy\n",
    "                alert_message = \"Animal Healthy\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{label} ({state})\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            \n",
    "            # Display alert message (or handle it as needed)\n",
    "            print(alert_message)\n",
    "    else:\n",
    "        print(\"No valid indexes returned by NMS.\")\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "586fbec0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m net \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov3.weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov3.cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mgetLayerNames()\n\u001b[1;32m----> 7\u001b[0m output_layers \u001b[38;5;241m=\u001b[39m [layer_names[i[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mgetUnconnectedOutLayers()]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load COCO labels\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco.names\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m net \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov3.weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov3.cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mgetLayerNames()\n\u001b[1;32m----> 7\u001b[0m output_layers \u001b[38;5;241m=\u001b[39m [layer_names[i[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mgetUnconnectedOutLayers()]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load COCO labels\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco.names\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "if not video_capture.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "    exit()\n",
    "\n",
    "def classify_animal_simulated(cropped_image):\n",
    "    # Simulate classification (replace with actual model or logic)\n",
    "    return np.random.choice(['Healthy', 'Injured', 'Dead'])\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"End of video or error reading frame.\")\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Debugging: Print the shape of the outputs\n",
    "    print(\"Output shapes:\")\n",
    "    for i, out in enumerate(outs):\n",
    "        print(f\"Layer {i}: shape = {out.shape}, type = {type(out)}\")\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            print(f\"Detection shape: {detection.shape}\")  # Debugging line\n",
    "            for obj in detection:\n",
    "                # Ensure obj is an array\n",
    "                if isinstance(obj, np.ndarray):\n",
    "                    if obj.ndim == 1 and obj.shape[0] >= 85:\n",
    "                        scores = obj[5:]\n",
    "                        class_id = np.argmax(scores)\n",
    "                        confidence = scores[class_id]\n",
    "\n",
    "                        if confidence > 0.5:  # Confidence threshold\n",
    "                            center_x = int(obj[0] * width)\n",
    "                            center_y = int(obj[1] * height)\n",
    "                            w = int(obj[2] * width)\n",
    "                            h = int(obj[3] * height)\n",
    "\n",
    "                            x = int(center_x - w / 2)\n",
    "                            y = int(center_y - h / 2)\n",
    "\n",
    "                            boxes.append([x, y, w, h])\n",
    "                            confidences.append(float(confidence))\n",
    "                            class_ids.append(class_id)\n",
    "                    else:\n",
    "                        print(\"Unexpected shape for obj:\", obj.shape)  # Debugging line\n",
    "                else:\n",
    "                    print(\"Unexpected type for obj:\", type(obj))  # Debugging line\n",
    "\n",
    "    # Perform non-max suppression to remove overlapping boxes\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    if indexes is not None:\n",
    "        indexes = indexes.flatten()  # Flatten the indexes array\n",
    "\n",
    "        # Draw the bounding boxes on the frame\n",
    "        for i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            cropped_img = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            # Simulate classification\n",
    "            state = classify_animal_simulated(cropped_img)\n",
    "\n",
    "            # Determine color based on simulated state\n",
    "            if state == \"Dead\":\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            elif state == \"Injured\":\n",
    "                color = (0, 255, 255)  # Yellow for injured\n",
    "                alert_message = \"Alert: Animal Injured\"\n",
    "            else:\n",
    "                color = (0, 255, 0)  # Green for healthy\n",
    "                alert_message = \"Animal Healthy\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{label} ({state})\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            \n",
    "            # Display alert message (or handle it as needed)\n",
    "            print(alert_message)\n",
    "    else:\n",
    "        print(\"No valid indexes returned by NMS.\")\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04cc2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40a865e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m unconnected_out_layers \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mgetUnconnectedOutLayers()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnconnected out layers:\u001b[39m\u001b[38;5;124m\"\u001b[39m, unconnected_out_layers)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType of unconnected out layers:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(unconnected_out_layers))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "unconnected_out_layers = net.getUnconnectedOutLayers()\n",
    "print(\"Unconnected out layers:\", unconnected_out_layers)\n",
    "print(\"Type of unconnected out layers:\", type(unconnected_out_layers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698645f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unconnected_out_layers = net.getUnconnectedOutLayers()\n",
    "\n",
    "# Extracting the indices based on the format of `unconnected_out_layers`\n",
    "if isinstance(unconnected_out_layers[0], list):\n",
    "    unconnected_out_layers = [i[0] for i in unconnected_out_layers]\n",
    "\n",
    "print(\"Processed unconnected out layers:\", unconnected_out_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52550846",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layers = [layer_names[i - 1] for i in unconnected_out_layers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d4b4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            for obj in detection:\n",
    "                scores = obj[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:  # Confidence threshold\n",
    "                    center_x = int(obj[0] * width)\n",
    "                    center_y = int(obj[1] * height)\n",
    "                    w = int(obj[2] * width)\n",
    "                    h = int(obj[3] * height)\n",
    "\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Draw the bounding boxes on the frame\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c772b3c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (111335337.py, line 91)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 91\u001b[1;36m\u001b[0m\n\u001b[1;33m    alert_message = \"Alert: Animal Injured\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "out_layer_indices = net.getUnconnectedOutLayers()\n",
    "\n",
    "# Check if the output layers are in a 2D array or not\n",
    "if len(out_layer_indices.shape) == 1:\n",
    "    output_layers = [layer_names[i - 1] for i in out_layer_indices]\n",
    "else:\n",
    "    output_layers = [layer_names[i[0] - 1] for i in out_layer_indices]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "if not video_capture.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "    exit()\n",
    "\n",
    "def classify_animal_simulated(cropped_image):\n",
    "    # Simulate classification (replace with actual model or logic)\n",
    "    return np.random.choice(['Healthy', 'Injured', 'Dead'])\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"End of video or error reading frame.\")\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            # The detection should be a list with at least 5 elements (box and confidence scores)\n",
    "            if detection.shape[0] >= 5:\n",
    "                # Extract the scores and class ID\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5:  # Confidence threshold\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "    # Perform non-max suppression to remove overlapping boxes\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    if indexes is not None:\n",
    "        indexes = indexes.flatten()  # Flatten the indexes array\n",
    "\n",
    "        # Draw the bounding boxes on the frame\n",
    "        for i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            cropped_img = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            # Simulate classification\n",
    "            state = classify_animal_simulated(cropped_img)\n",
    "\n",
    "            # Determine color based on simulated state\n",
    "            if state == \"Dead\":\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            elif state == \"Injured\":\n",
    "                color = (0, 255, 255)  # Yellow\n",
    "                            alert_message = \"Alert: Animal Injured\"\n",
    "            else:\n",
    "                color = (0, 255, 0)  # Green for healthy\n",
    "                alert_message = \"Animal Healthy\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{label} ({state})\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            \n",
    "            # Display alert message (or handle it as needed)\n",
    "            print(alert_message)\n",
    "            else:\n",
    "                print(\"No valid indexes returned by NMS.\")\n",
    "\n",
    "            # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "317ea148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b303c42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Emergency Alert: Animal Dead\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Alert: Animal Injured\n",
      "Emergency Alert: Animal Dead\n",
      "Animal Healthy\n",
      "Alert: Animal Injured\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "def classify_animal_simulated(cropped_image):\n",
    "    # Simulate classification (replace with actual model or logic)\n",
    "    return np.random.choice(['Healthy', 'Injured', 'Dead'])\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            # Extract bounding box and confidence\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:  # Confidence threshold\n",
    "                # Get bounding box coordinates\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Calculate the top left corner of the bounding box\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Remove overlapping bounding boxes with Non-Maximum Suppression\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Draw the bounding boxes on the frame\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            cropped_img = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            # Simulate classification\n",
    "            state = classify_animal_simulated(cropped_img)\n",
    "\n",
    "            # Determine color based on simulated state\n",
    "            if state == \"Dead\":\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            elif state == \"Injured\":\n",
    "                color = (0, 255, 255)  # Yellow for injured\n",
    "                alert_message = \"Alert: Animal Injured\"\n",
    "            else:\n",
    "                color = (0, 255, 0)  # Green for healthy\n",
    "                alert_message = \"Animal Healthy\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{label} ({state})\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            \n",
    "            # Display alert message (or handle it as needed)\n",
    "            print(alert_message)  # Example alert system; replace with actual notification system if needed\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#love you code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b17140",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'MultiTracker_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m video_capture \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhackx.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize object tracker\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m tracker \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mMultiTracker_create()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Dictionary to store object states and their histories\u001b[39;00m\n\u001b[0;32m     21\u001b[0m object_states \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'MultiTracker_create'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "# Initialize object tracker\n",
    "tracker = cv2.MultiTracker_create()\n",
    "\n",
    "# Dictionary to store object states and their histories\n",
    "object_states = defaultdict(list)\n",
    "\n",
    "def classify_animal_simulated(cropped_image):\n",
    "    # Simulate classification (replace with actual model or logic)\n",
    "    return np.random.choice(['Healthy', 'Injured', 'Dead'])\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:  # Confidence threshold\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Update the tracker with detected objects\n",
    "    new_boxes = []\n",
    "    for i in indexes.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        new_boxes.append((x, y, w, h))\n",
    "    \n",
    "    # If tracking is initialized, update trackers\n",
    "    if len(tracker.getObjects()) > 0:\n",
    "        success, boxes = tracker.update(frame)\n",
    "        for i, box in enumerate(boxes):\n",
    "            x, y, w, h = list(map(int, box))\n",
    "            cropped_img = frame[y:y+h, x:x+w]\n",
    "            state = classify_animal_simulated(cropped_img)\n",
    "            object_states[i].append(state)\n",
    "            \n",
    "            # Determine final state based on majority voting\n",
    "            final_state = max(set(object_states[i]), key=object_states[i].count)\n",
    "            \n",
    "            # Determine color based on final state\n",
    "            if final_state == \"Dead\":\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            elif final_state == \"Injured\":\n",
    "                color = (0, 255, 255)  # Yellow for injured\n",
    "                alert_message = \"Alert: Animal Injured\"\n",
    "            else:\n",
    "                color = (0, 255, 0)  # Green for healthy\n",
    "                alert_message = \"Animal Healthy\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{classes[class_ids[i]]} ({final_state})\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            print(alert_message)\n",
    "\n",
    "    # Initialize new trackers for detected objects\n",
    "    if len(new_boxes) > 0:\n",
    "        for box in new_boxes:\n",
    "            tracker.add(cv2.TrackerKCF_create(), frame, tuple(box))\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#love the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787043e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "# Initialize object tracker\n",
    "tracker = cv2.MultiTracker_create()\n",
    "\n",
    "# Dictionary to store object states and their histories\n",
    "object_states = defaultdict(list)\n",
    "\n",
    "def classify_animal_simulated(cropped_image):\n",
    "    # Simulate classification (replace with actual model or logic)\n",
    "    return np.random.choice(['Healthy', 'Injured', 'Dead'])\n",
    "\n",
    "def aggregate_classifications(states):\n",
    "    # Aggregate classifications using majority voting\n",
    "    return Counter(states).most_common(1)[0][0]\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:  # Confidence threshold\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Update the tracker with detected objects\n",
    "    new_boxes = []\n",
    "    for i in indexes.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        new_boxes.append((x, y, w, h))\n",
    "    \n",
    "    # If tracking is initialized, update trackers\n",
    "    if len(tracker.getObjects()) > 0:\n",
    "        success, tracked_boxes = tracker.update(frame)\n",
    "        for i, box in enumerate(tracked_boxes):\n",
    "            x, y, w, h = list(map(int, box))\n",
    "            cropped_img = frame[y:y+h, x:x+w]\n",
    "            state = classify_animal_simulated(cropped_img)\n",
    "            object_states[i].append(state)\n",
    "            \n",
    "            # Determine final state based on majority voting\n",
    "            final_state = aggregate_classifications(object_states[i])\n",
    "            \n",
    "            # Determine color based on final state\n",
    "            if final_state == \"Dead\":\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            elif final_state == \"Injured\":\n",
    "                color = (0, 255, 255)  # Yellow for injured\n",
    "                alert_message = \"Alert: Animal Injured\"\n",
    "            else:\n",
    "                color = (0, 255, 0)  # Green for healthy\n",
    "                alert_message = \"Animal Healthy\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{classes[class_ids[i]]} ({final_state})\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            print(alert_message)\n",
    "\n",
    "    # Initialize new trackers for detected objects\n",
    "    if len(new_boxes) > 0:\n",
    "        for box in new_boxes:\n",
    "            tracker.add(cv2.TrackerKCF_create(), frame, tuple(box))\n",
    "            object_states[len(tracker.getObjects()) - 1] = []  # Initialize state history for new objects\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "103583cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'MultiTracker_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m video_capture \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhackx.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize object tracker\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m tracker \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mMultiTracker_create()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Dictionary to store object states and movement history\u001b[39;00m\n\u001b[0;32m     21\u001b[0m object_movement \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_position\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe_counter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'MultiTracker_create'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "# Initialize object tracker\n",
    "tracker = cv2.MultiTracker_create()\n",
    "\n",
    "# Dictionary to store object states and movement history\n",
    "object_movement = defaultdict(lambda: {'last_position': None, 'frame_counter': 0})\n",
    "\n",
    "def classify_animal_simulated(cropped_image):\n",
    "    # Simulate classification (replace with actual model or logic)\n",
    "    return np.random.choice(['Healthy', 'Injured'])\n",
    "\n",
    "def classify_object(x, y, w, h):\n",
    "    # This function will simulate classification of an object\n",
    "    # Replace this with real classification logic\n",
    "    return classify_animal_simulated(None)\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:  # Confidence threshold\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Update the tracker with detected objects\n",
    "    new_boxes = []\n",
    "    for i in indexes.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        new_boxes.append((x, y, w, h))\n",
    "    \n",
    "    # If tracking is initialized, update trackers\n",
    "    if len(tracker.getObjects()) > 0:\n",
    "        success, tracked_boxes = tracker.update(frame)\n",
    "        for i, box in enumerate(tracked_boxes):\n",
    "            x, y, w, h = list(map(int, box))\n",
    "            current_position = (x, y, w, h)\n",
    "            \n",
    "            # Movement analysis\n",
    "            if object_movement[i]['last_position'] is not None:\n",
    "                last_position = object_movement[i]['last_position']\n",
    "                if abs(current_position[0] - last_position[0]) < 10 and abs(current_position[1] - last_position[1]) < 10:\n",
    "                    object_movement[i]['frame_counter'] += 1\n",
    "                else:\n",
    "                    object_movement[i]['frame_counter'] = 0\n",
    "                object_movement[i]['last_position'] = current_position\n",
    "            else:\n",
    "                object_movement[i]['last_position'] = current_position\n",
    "\n",
    "            if object_movement[i]['frame_counter'] > 50:  # Adjust the threshold based on your video\n",
    "                final_state = \"Dead\"\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            else:\n",
    "                final_state = classify_object(x, y, w, h)\n",
    "                if final_state == \"Dead\":\n",
    "                    color = (0, 0, 255)  # Red for dead\n",
    "                    alert_message = \"Emergency Alert: Animal Dead\"\n",
    "                elif final_state == \"Injured\":\n",
    "                    color = (0, 255, 255)  # Yellow for injured\n",
    "                    alert_message = \"Alert: Animal Injured\"\n",
    "                else:\n",
    "                    color = (0, 255, 0)  # Green for healthy\n",
    "                    alert_message = \"Animal Healthy\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{classes[class_ids[i]]} ({final_state})\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            print(alert_message)\n",
    "\n",
    "    # Initialize new trackers for detected objects\n",
    "    if len(new_boxes) > 0:\n",
    "        for box in new_boxes:\n",
    "            tracker.add(cv2.TrackerKCF_create(), frame, tuple(box))\n",
    "            object_movement[len(tracker.getObjects()) - 1] = {'last_position': None, 'frame_counter': 0}\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce6da9ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'TrackerKCF_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m new_boxes\u001b[38;5;241m.\u001b[39mappend((x, y, w, h))\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Create a new tracker for each detected object\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m tracker \u001b[38;5;241m=\u001b[39m create_tracker()  \u001b[38;5;66;03m# Use the universal tracker creation function\u001b[39;00m\n\u001b[0;32m     87\u001b[0m tracker\u001b[38;5;241m.\u001b[39minit(frame, (x, y, w, h))\n\u001b[0;32m     88\u001b[0m new_trackers\u001b[38;5;241m.\u001b[39mappend(tracker)\n",
      "Cell \u001b[1;32mIn[13], line 35\u001b[0m, in \u001b[0;36mcreate_tracker\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_tracker\u001b[39m():\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mTrackerKCF_create()  \u001b[38;5;66;03m# For OpenCV 4.x\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mTrackerKCF_create()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'TrackerKCF_create'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "# Initialize individual trackers\n",
    "trackers = []\n",
    "\n",
    "# Dictionary to store object states and movement history\n",
    "object_movement = defaultdict(lambda: {'last_position': None, 'frame_counter': 0})\n",
    "\n",
    "def classify_animal_simulated(cropped_image):\n",
    "    # Simulate classification (replace with actual model or logic)\n",
    "    return np.random.choice(['Healthy', 'Injured'])\n",
    "\n",
    "def classify_object(x, y, w, h):\n",
    "    # This function will simulate classification of an object\n",
    "    # Replace this with real classification logic\n",
    "    return classify_animal_simulated(None)\n",
    "\n",
    "# Initialize a tracker\n",
    "def create_tracker():\n",
    "    if cv2.__version__.startswith('4'):\n",
    "        return cv2.TrackerKCF_create()  # For OpenCV 4.x\n",
    "    else:\n",
    "        return cv2.TrackerKCF_create()  # For OpenCV < 4.x\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:  # Confidence threshold\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Create new trackers\n",
    "    new_trackers = []\n",
    "    new_boxes = []\n",
    "    for i in indexes.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        new_boxes.append((x, y, w, h))\n",
    "        \n",
    "        # Create a new tracker for each detected object\n",
    "        tracker = create_tracker()  # Use the universal tracker creation function\n",
    "        tracker.init(frame, (x, y, w, h))\n",
    "        new_trackers.append(tracker)\n",
    "\n",
    "    # Initialize trackers for new detections\n",
    "    trackers.extend(new_trackers)\n",
    "\n",
    "    # Update existing trackers\n",
    "    updated_trackers = []\n",
    "    if len(trackers) > 0:\n",
    "        for i, tracker in enumerate(trackers):\n",
    "            success, box = tracker.update(frame)\n",
    "            if success:\n",
    "                x, y, w, h = list(map(int, box))\n",
    "                current_position = (x, y, w, h)\n",
    "                \n",
    "                # Movement analysis\n",
    "                if object_movement[i]['last_position'] is not None:\n",
    "                    last_position = object_movement[i]['last_position']\n",
    "                    if abs(current_position[0] - last_position[0]) < 10 and abs(current_position[1] - last_position[1]) < 10:\n",
    "                        object_movement[i]['frame_counter'] += 1\n",
    "                    else:\n",
    "                        object_movement[i]['frame_counter'] = 0\n",
    "                    object_movement[i]['last_position'] = current_position\n",
    "                else:\n",
    "                    object_movement[i]['last_position'] = current_position\n",
    "\n",
    "                if object_movement[i]['frame_counter'] > 50:  # Adjust the threshold based on your video\n",
    "                    final_state = \"Dead\"\n",
    "                    color = (0, 0, 255)  # Red for dead\n",
    "                    alert_message = \"Emergency Alert: Animal Dead\"\n",
    "                else:\n",
    "                    final_state = classify_object(x, y, w, h)\n",
    "                    if final_state == \"Dead\":\n",
    "                        color = (0, 0, 255)  # Red for dead\n",
    "                        alert_message = \"Emergency Alert: Animal Dead\"\n",
    "                    elif final_state == \"Injured\":\n",
    "                        color = (0, 255, 255)  # Yellow for injured\n",
    "                        alert_message = \"Alert: Animal Injured\"\n",
    "                    else:\n",
    "                        color = (0, 255, 0)  # Green for healthy\n",
    "                        alert_message = \"Animal Healthy\"\n",
    "\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(frame, f\"{classes[class_ids[i]]} ({final_state})\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "                print(alert_message)\n",
    "\n",
    "                updated_trackers.append(tracker)  # Only keep updated trackers\n",
    "\n",
    "    # Replace old trackers with updated trackers\n",
    "    trackers = updated_trackers\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85166fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def create_tracker():\n",
    "    if cv2.__version__.startswith('4'):\n",
    "        return cv2.TrackerCSRT_create()  # Use CSRT tracker for OpenCV 4.x\n",
    "    else:\n",
    "        return cv2.TrackerKCF_create()  # For older versions of OpenCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f98eeb26",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'TrackerKCF_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m new_boxes\u001b[38;5;241m.\u001b[39mappend((x, y, w, h))\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Create a new tracker for each detected object\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m tracker \u001b[38;5;241m=\u001b[39m create_tracker()  \u001b[38;5;66;03m# Use the universal tracker creation function\u001b[39;00m\n\u001b[0;32m     87\u001b[0m tracker\u001b[38;5;241m.\u001b[39minit(frame, (x, y, w, h))\n\u001b[0;32m     88\u001b[0m new_trackers\u001b[38;5;241m.\u001b[39mappend(tracker)\n",
      "Cell \u001b[1;32mIn[23], line 35\u001b[0m, in \u001b[0;36mcreate_tracker\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_tracker\u001b[39m():\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mTrackerKCF_create()  \u001b[38;5;66;03m# For OpenCV 4.x\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mTrackerKCF_create()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'TrackerKCF_create'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "# Initialize individual trackers\n",
    "trackers = []\n",
    "\n",
    "# Dictionary to store object states and movement history\n",
    "object_movement = defaultdict(lambda: {'last_position': None, 'frame_counter': 0})\n",
    "\n",
    "def classify_animal_simulated(cropped_image):\n",
    "    # Simulate classification (replace with actual model or logic)\n",
    "    return np.random.choice(['Healthy', 'Injured'])\n",
    "\n",
    "def classify_object(x, y, w, h):\n",
    "    # This function will simulate classification of an object\n",
    "    # Replace this with real classification logic\n",
    "    return classify_animal_simulated(None)\n",
    "\n",
    "# Initialize a tracker\n",
    "def create_tracker():\n",
    "    if cv2.__version__.startswith('4'):\n",
    "        return cv2.TrackerKCF_create()  # For OpenCV 4.x\n",
    "    else:\n",
    "        return cv2.TrackerKCF_create()  # For OpenCV < 4.x\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:  # Confidence threshold\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Create new trackers\n",
    "    new_trackers = []\n",
    "    new_boxes = []\n",
    "    for i in indexes.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        new_boxes.append((x, y, w, h))\n",
    "        \n",
    "        # Create a new tracker for each detected object\n",
    "        tracker = create_tracker()  # Use the universal tracker creation function\n",
    "        tracker.init(frame, (x, y, w, h))\n",
    "        new_trackers.append(tracker)\n",
    "\n",
    "    # Initialize trackers for new detections\n",
    "    trackers.extend(new_trackers)\n",
    "\n",
    "    # Update existing trackers\n",
    "    updated_trackers = []\n",
    "    if len(trackers) > 0:\n",
    "        for i, tracker in enumerate(trackers):\n",
    "            success, box = tracker.update(frame)\n",
    "            if success:\n",
    "                x, y, w, h = list(map(int, box))\n",
    "                current_position = (x, y, w, h)\n",
    "                \n",
    "                # Movement analysis\n",
    "                if object_movement[i]['last_position'] is not None:\n",
    "                    last_position = object_movement[i]['last_position']\n",
    "                    if abs(current_position[0] - last_position[0]) < 10 and abs(current_position[1] - last_position[1]) < 10:\n",
    "                        object_movement[i]['frame_counter'] += 1\n",
    "                    else:\n",
    "                        object_movement[i]['frame_counter'] = 0\n",
    "                    object_movement[i]['last_position'] = current_position\n",
    "                else:\n",
    "                    object_movement[i]['last_position'] = current_position\n",
    "\n",
    "                if object_movement[i]['frame_counter'] > 50:  # Adjust the threshold based on your video\n",
    "                    final_state = \"Dead\"\n",
    "                    color = (0, 0, 255)  # Red for dead\n",
    "                    alert_message = \"Emergency Alert: Animal Dead\"\n",
    "                else:\n",
    "                    final_state = classify_object(x, y, w, h)\n",
    "                    if final_state == \"Dead\":\n",
    "                        color = (0, 0, 255)  # Red for dead\n",
    "                        alert_message = \"Emergency Alert: Animal Dead\"\n",
    "                    elif final_state == \"Injured\":\n",
    "                        color = (0, 255, 255)  # Yellow for injured\n",
    "                        alert_message = \"Alert: Animal Injured\"\n",
    "                    else:\n",
    "                        color = (0, 255, 0)  # Green for healthy\n",
    "                        alert_message = \"Animal Healthy\"\n",
    "\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(frame, f\"{classes[class_ids[i]]} ({final_state})\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "                print(alert_message)\n",
    "\n",
    "                updated_trackers.append(tracker)  # Only keep updated trackers\n",
    "\n",
    "    # Replace old trackers with updated trackers\n",
    "    trackers = updated_trackers\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0decccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in c:\\users\\91962\\appdata\\roaming\\python\\python311\\site-packages (4.10.0.84)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "                                              0.0/38.8 MB ? eta -:--:--\n",
      "                                              0.2/38.8 MB 12.2 MB/s eta 0:00:04\n",
      "                                              0.3/38.8 MB 4.0 MB/s eta 0:00:10\n",
      "                                              0.4/38.8 MB 2.8 MB/s eta 0:00:14\n",
      "                                              0.5/38.8 MB 2.4 MB/s eta 0:00:17\n",
      "                                              0.5/38.8 MB 2.5 MB/s eta 0:00:16\n",
      "                                              0.6/38.8 MB 2.2 MB/s eta 0:00:18\n",
      "                                              0.7/38.8 MB 2.1 MB/s eta 0:00:18\n",
      "                                              0.8/38.8 MB 2.1 MB/s eta 0:00:18\n",
      "                                              0.8/38.8 MB 2.0 MB/s eta 0:00:19\n",
      "                                              0.9/38.8 MB 2.0 MB/s eta 0:00:20\n",
      "     -                                        1.0/38.8 MB 1.9 MB/s eta 0:00:20\n",
      "     -                                        1.1/38.8 MB 2.0 MB/s eta 0:00:19\n",
      "     -                                        1.1/38.8 MB 1.9 MB/s eta 0:00:21\n",
      "     -                                        1.2/38.8 MB 1.9 MB/s eta 0:00:20\n",
      "     -                                        1.3/38.8 MB 1.9 MB/s eta 0:00:20\n",
      "     -                                        1.4/38.8 MB 1.9 MB/s eta 0:00:21\n",
      "     -                                        1.4/38.8 MB 1.8 MB/s eta 0:00:21\n",
      "     -                                        1.5/38.8 MB 1.8 MB/s eta 0:00:21\n",
      "     -                                        1.6/38.8 MB 1.8 MB/s eta 0:00:21\n",
      "     -                                        1.6/38.8 MB 1.8 MB/s eta 0:00:21\n",
      "     -                                        1.7/38.8 MB 1.8 MB/s eta 0:00:21\n",
      "     -                                        1.8/38.8 MB 1.8 MB/s eta 0:00:22\n",
      "     -                                        1.9/38.8 MB 1.7 MB/s eta 0:00:22\n",
      "     -                                        1.9/38.8 MB 1.7 MB/s eta 0:00:22\n",
      "     --                                       2.0/38.8 MB 1.7 MB/s eta 0:00:22\n",
      "     --                                       2.1/38.8 MB 1.7 MB/s eta 0:00:22\n",
      "     --                                       2.1/38.8 MB 1.7 MB/s eta 0:00:22\n",
      "     --                                       2.2/38.8 MB 1.7 MB/s eta 0:00:22\n",
      "     --                                       2.3/38.8 MB 1.7 MB/s eta 0:00:22\n",
      "     --                                       2.3/38.8 MB 1.7 MB/s eta 0:00:22\n",
      "     --                                       2.4/38.8 MB 1.7 MB/s eta 0:00:22\n",
      "     --                                       2.5/38.8 MB 1.7 MB/s eta 0:00:22\n",
      "     --                                       2.5/38.8 MB 1.7 MB/s eta 0:00:22\n",
      "     --                                       2.5/38.8 MB 1.6 MB/s eta 0:00:23\n",
      "     --                                       2.7/38.8 MB 1.6 MB/s eta 0:00:23\n",
      "     --                                       2.8/38.8 MB 1.7 MB/s eta 0:00:22\n",
      "     --                                       2.8/38.8 MB 1.7 MB/s eta 0:00:22\n",
      "     --                                       2.8/38.8 MB 1.7 MB/s eta 0:00:22\n",
      "     ---                                      2.9/38.8 MB 1.6 MB/s eta 0:00:23\n",
      "     ---                                      3.0/38.8 MB 1.6 MB/s eta 0:00:23\n",
      "     ---                                      3.1/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     ---                                      3.2/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     ---                                      3.2/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     ---                                      3.2/38.8 MB 1.6 MB/s eta 0:00:23\n",
      "     ---                                      3.3/38.8 MB 1.6 MB/s eta 0:00:23\n",
      "     ---                                      3.4/38.8 MB 1.6 MB/s eta 0:00:23\n",
      "     ---                                      3.4/38.8 MB 1.6 MB/s eta 0:00:23\n",
      "     ---                                      3.5/38.8 MB 1.6 MB/s eta 0:00:23\n",
      "     ---                                      3.6/38.8 MB 1.6 MB/s eta 0:00:23\n",
      "     ---                                      3.6/38.8 MB 1.5 MB/s eta 0:00:23\n",
      "     ---                                      3.7/38.8 MB 1.6 MB/s eta 0:00:23\n",
      "     ---                                      3.9/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     ---                                      3.9/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     ----                                     3.9/38.8 MB 1.6 MB/s eta 0:00:23\n",
      "     ----                                     4.0/38.8 MB 1.5 MB/s eta 0:00:23\n",
      "     ----                                     4.2/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     ----                                     4.3/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     ----                                     4.3/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     ----                                     4.3/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     ----                                     4.4/38.8 MB 1.6 MB/s eta 0:00:23\n",
      "     ----                                     4.5/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     ----                                     4.5/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     ----                                     4.5/38.8 MB 1.5 MB/s eta 0:00:23\n",
      "     ----                                     4.6/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     ----                                     4.8/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     -----                                    4.9/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     -----                                    4.9/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     -----                                    5.0/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     -----                                    5.1/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     -----                                    5.2/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     -----                                    5.2/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     -----                                    5.3/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     -----                                    5.4/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     -----                                    5.4/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     -----                                    5.5/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     -----                                    5.6/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     -----                                    5.6/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     -----                                    5.6/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     -----                                    5.7/38.8 MB 1.6 MB/s eta 0:00:22\n",
      "     ------                                   5.8/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     ------                                   5.9/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     ------                                   5.9/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     ------                                   6.0/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     ------                                   6.1/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     ------                                   6.2/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     ------                                   6.2/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     ------                                   6.3/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     ------                                   6.3/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     ------                                   6.4/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     ------                                   6.5/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     ------                                   6.6/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     ------                                   6.6/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     ------                                   6.7/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     ------                                   6.8/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     -------                                  6.8/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     -------                                  6.9/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     -------                                  7.0/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     -------                                  7.0/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     -------                                  7.1/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     -------                                  7.2/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     -------                                  7.3/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     -------                                  7.3/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     -------                                  7.4/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     -------                                  7.5/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     -------                                  7.5/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     -------                                  7.6/38.8 MB 1.6 MB/s eta 0:00:21\n",
      "     -------                                  7.6/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     -------                                  7.7/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     --------                                 7.8/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     --------                                 7.8/38.8 MB 1.5 MB/s eta 0:00:21\n",
      "     --------                                 7.8/38.8 MB 1.5 MB/s eta 0:00:20\n",
      "     --------                                 7.9/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     --------                                 8.0/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     --------                                 8.1/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     --------                                 8.2/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     --------                                 8.2/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     --------                                 8.3/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     --------                                 8.4/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     --------                                 8.4/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     --------                                 8.5/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     --------                                 8.6/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     --------                                 8.7/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     ---------                                8.7/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     ---------                                8.8/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     ---------                                8.9/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     ---------                                8.9/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     ---------                                9.0/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     ---------                                9.1/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     ---------                                9.1/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     ---------                                9.2/38.8 MB 1.5 MB/s eta 0:00:20\n",
      "     ---------                                9.3/38.8 MB 1.6 MB/s eta 0:00:20\n",
      "     ---------                                9.3/38.8 MB 1.6 MB/s eta 0:00:19\n",
      "     ---------                                9.4/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     ---------                                9.5/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     ---------                                9.5/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     ---------                                9.6/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     ----------                               9.7/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     ----------                               9.8/38.8 MB 1.6 MB/s eta 0:00:19\n",
      "     ----------                               9.8/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     ----------                               9.9/38.8 MB 1.6 MB/s eta 0:00:19\n",
      "     ----------                               10.0/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     ----------                               10.1/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     ----------                               10.1/38.8 MB 1.6 MB/s eta 0:00:19\n",
      "     ----------                               10.2/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     ----------                               10.3/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     ----------                               10.3/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     ----------                               10.4/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     ----------                               10.5/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     ----------                               10.5/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     ----------                               10.6/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -----------                              10.7/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -----------                              10.8/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -----------                              10.9/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -----------                              10.9/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -----------                              11.0/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -----------                              11.1/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -----------                              11.2/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -----------                              11.2/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -----------                              11.3/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -----------                              11.3/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -----------                              11.4/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -----------                              11.4/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -----------                              11.6/38.8 MB 1.5 MB/s eta 0:00:19\n",
      "     ------------                             11.7/38.8 MB 1.5 MB/s eta 0:00:18\n",
      "     ------------                             11.8/38.8 MB 1.5 MB/s eta 0:00:18\n",
      "     ------------                             11.8/38.8 MB 1.5 MB/s eta 0:00:18\n",
      "     ------------                             11.9/38.8 MB 1.5 MB/s eta 0:00:18\n",
      "     ------------                             12.0/38.8 MB 1.5 MB/s eta 0:00:18\n",
      "     ------------                             12.0/38.8 MB 1.5 MB/s eta 0:00:18\n",
      "     ------------                             12.1/38.8 MB 1.5 MB/s eta 0:00:18\n",
      "     ------------                             12.1/38.8 MB 1.5 MB/s eta 0:00:18\n",
      "     ------------                             12.2/38.8 MB 1.5 MB/s eta 0:00:18\n",
      "     ------------                             12.2/38.8 MB 1.5 MB/s eta 0:00:18\n",
      "     ------------                             12.3/38.8 MB 1.5 MB/s eta 0:00:18\n",
      "     ------------                             12.3/38.8 MB 1.5 MB/s eta 0:00:18\n",
      "     ------------                             12.5/38.8 MB 1.5 MB/s eta 0:00:18\n",
      "     ------------                             12.5/38.8 MB 1.5 MB/s eta 0:00:18\n",
      "     -------------                            12.6/38.8 MB 1.5 MB/s eta 0:00:18\n",
      "     -------------                            12.8/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     -------------                            12.9/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     -------------                            12.9/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     -------------                            13.0/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     -------------                            13.1/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     -------------                            13.1/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     -------------                            13.2/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     -------------                            13.3/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     -------------                            13.4/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     -------------                            13.4/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     -------------                            13.5/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           13.6/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           13.6/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           13.7/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           13.8/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           13.8/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           13.8/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           13.9/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           14.0/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           14.1/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     --------------                           14.2/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     --------------                           14.2/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           14.2/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           14.3/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           14.4/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           14.4/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           14.4/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           14.4/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           14.4/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           14.4/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     --------------                           14.5/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------                          14.6/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------                          14.6/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------                          14.7/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------                          14.8/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------                          15.0/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------                          15.1/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------                          15.1/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ---------------                          15.1/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ---------------                          15.2/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------                          15.2/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------                          15.3/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------                          15.4/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ---------------                          15.4/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ---------------                          15.4/38.8 MB 1.4 MB/s eta 0:00:17\n",
      "     ----------------                         15.5/38.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ----------------                         15.6/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------                         15.6/38.8 MB 1.4 MB/s eta 0:00:17\n",
      "     ----------------                         15.7/38.8 MB 1.4 MB/s eta 0:00:16\n",
      "     ----------------                         15.9/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------                         15.9/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------                         16.1/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------                         16.1/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------                         16.2/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------                         16.3/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------                         16.4/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------                         16.4/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     -----------------                        16.5/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     -----------------                        16.6/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     -----------------                        16.7/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     -----------------                        16.7/38.8 MB 1.5 MB/s eta 0:00:16\n",
      "     -----------------                        16.8/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -----------------                        16.8/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -----------------                        16.9/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -----------------                        17.0/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -----------------                        17.0/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -----------------                        17.1/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -----------------                        17.2/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -----------------                        17.2/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -----------------                        17.3/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -----------------                        17.4/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -----------------                        17.4/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------                       17.5/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------                       17.6/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------                       17.6/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------                       17.7/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------                       17.8/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------                       17.9/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------                       17.9/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------                       18.0/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------                       18.1/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------                       18.1/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------                       18.2/38.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------                       18.2/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     ------------------                       18.3/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     ------------------                       18.4/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      18.4/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      18.5/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      18.5/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      18.6/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      18.6/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      18.7/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      18.8/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      18.8/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      18.9/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      19.0/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      19.0/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      19.1/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      19.1/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      19.2/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      19.3/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------                      19.3/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------                     19.4/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------                     19.5/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------                     19.6/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------                     19.6/38.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------                     19.7/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     --------------------                     19.8/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     --------------------                     19.9/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     --------------------                     19.9/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     --------------------                     20.0/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     --------------------                     20.0/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     --------------------                     20.1/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     --------------------                     20.2/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     --------------------                     20.3/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     --------------------                     20.3/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------                    20.4/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------                    20.5/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------                    20.6/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------                    20.6/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------                    20.6/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------                    20.7/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------                    20.8/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------                    20.9/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------                    21.0/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------                    21.1/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------                    21.1/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------                    21.2/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------                    21.2/38.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------                    21.3/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     ----------------------                   21.4/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     ----------------------                   21.5/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     ----------------------                   21.6/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     ----------------------                   21.7/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     ----------------------                   21.7/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     ----------------------                   21.8/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     ----------------------                   21.9/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     ----------------------                   21.9/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     ----------------------                   22.0/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     ----------------------                   22.1/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     ----------------------                   22.1/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     ----------------------                   22.2/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     -----------------------                  22.3/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     -----------------------                  22.3/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     -----------------------                  22.4/38.8 MB 1.5 MB/s eta 0:00:12\n",
      "     -----------------------                  22.5/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     -----------------------                  22.6/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     -----------------------                  22.7/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     -----------------------                  22.7/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     -----------------------                  22.8/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     -----------------------                  22.9/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     -----------------------                  22.9/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     -----------------------                  23.0/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     -----------------------                  23.1/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     -----------------------                  23.2/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     -----------------------                  23.2/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     ------------------------                 23.3/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     ------------------------                 23.4/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     ------------------------                 23.5/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     ------------------------                 23.5/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     ------------------------                 23.6/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     ------------------------                 23.7/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     ------------------------                 23.7/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     ------------------------                 23.8/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     ------------------------                 23.9/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     ------------------------                 24.0/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     ------------------------                 24.0/38.8 MB 1.5 MB/s eta 0:00:11\n",
      "     ------------------------                 24.1/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     ------------------------                 24.2/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     -------------------------                24.2/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     -------------------------                24.3/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     -------------------------                24.4/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     -------------------------                24.5/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     -------------------------                24.5/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     -------------------------                24.6/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     -------------------------                24.7/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     -------------------------                24.7/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     -------------------------                24.8/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     -------------------------                24.9/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     -------------------------                24.9/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     -------------------------                25.0/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     -------------------------                25.0/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     -------------------------                25.1/38.8 MB 1.5 MB/s eta 0:00:10\n",
      "     --------------------------               25.2/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --------------------------               25.3/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --------------------------               25.3/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --------------------------               25.4/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --------------------------               25.5/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --------------------------               25.7/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --------------------------               25.7/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --------------------------               25.7/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --------------------------               25.7/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --------------------------               25.8/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --------------------------               25.8/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --------------------------               25.8/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --------------------------               25.9/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --------------------------               25.9/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --------------------------               26.0/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     --------------------------               26.1/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     ---------------------------              26.2/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     ---------------------------              26.2/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     ---------------------------              26.2/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     ---------------------------              26.3/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     ---------------------------              26.4/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     ---------------------------              26.5/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     ---------------------------              26.5/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     ---------------------------              26.5/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     ---------------------------              26.6/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     ---------------------------              26.6/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     ---------------------------              26.7/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     ---------------------------              26.8/38.8 MB 1.5 MB/s eta 0:00:09\n",
      "     ---------------------------              27.0/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ---------------------------              27.1/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------             27.2/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------             27.3/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------             27.3/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------             27.4/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------             27.5/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------             27.5/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------             27.6/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------             27.6/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------             27.7/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------             27.7/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------             27.8/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------             27.9/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------             28.0/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     -----------------------------            28.1/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     -----------------------------            28.2/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     -----------------------------            28.3/38.8 MB 1.5 MB/s eta 0:00:08\n",
      "     -----------------------------            28.3/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     -----------------------------            28.4/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     -----------------------------            28.4/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     -----------------------------            28.5/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     -----------------------------            28.6/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     -----------------------------            28.7/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     -----------------------------            28.8/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     -----------------------------            28.9/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     -----------------------------            28.9/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     -----------------------------            28.9/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     -----------------------------            29.0/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     -----------------------------            29.0/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     -----------------------------            29.1/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------           29.2/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------           29.3/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------           29.3/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------           29.4/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------           29.5/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------           29.6/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------           29.7/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------           29.8/38.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------           29.9/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     ------------------------------           30.0/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     ------------------------------           30.0/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------          30.1/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------          30.2/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------          30.2/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------          30.3/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------          30.4/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------          30.4/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------          30.5/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------          30.6/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------          30.6/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------          30.7/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------          30.8/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------          30.9/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------          30.9/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     --------------------------------         31.0/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     --------------------------------         31.1/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     --------------------------------         31.1/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     --------------------------------         31.2/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     --------------------------------         31.3/38.8 MB 1.5 MB/s eta 0:00:06\n",
      "     --------------------------------         31.3/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------         31.4/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------         31.5/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------         31.6/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------         31.6/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------         31.8/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------         31.8/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------         31.9/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------         32.0/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------        32.0/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------        32.1/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------        32.2/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------        32.2/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------        32.2/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------        32.2/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------        32.3/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------        32.4/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------        32.5/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------        32.6/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------        32.7/38.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------        32.8/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ---------------------------------        32.9/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------       33.0/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------       33.1/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------       33.1/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------       33.2/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------       33.2/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------       33.4/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------       33.4/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------       33.4/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------       33.5/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------       33.6/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------       33.7/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------       33.7/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------       33.8/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------       33.9/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     -----------------------------------      34.0/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     -----------------------------------      34.0/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     -----------------------------------      34.1/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     -----------------------------------      34.2/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     -----------------------------------      34.3/38.8 MB 1.5 MB/s eta 0:00:04\n",
      "     -----------------------------------      34.3/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     -----------------------------------      34.4/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     -----------------------------------      34.4/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     -----------------------------------      34.6/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     -----------------------------------      34.6/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     -----------------------------------      34.7/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     -----------------------------------      34.8/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     -----------------------------------      34.8/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------     34.9/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------     35.0/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------     35.1/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------     35.1/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------     35.2/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------     35.3/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------     35.3/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------     35.4/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------     35.4/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------     35.5/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------     35.6/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------     35.7/38.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------     35.8/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    35.9/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    36.0/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    36.0/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    36.1/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    36.1/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    36.2/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    36.3/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    36.4/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    36.4/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    36.4/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    36.5/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    36.6/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    36.7/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    36.7/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    36.8/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------    36.8/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     --------------------------------------   36.8/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     --------------------------------------   36.8/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     --------------------------------------   36.9/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     --------------------------------------   37.0/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     --------------------------------------   37.0/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     --------------------------------------   37.0/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     --------------------------------------   37.1/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     --------------------------------------   37.2/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     --------------------------------------   37.2/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     --------------------------------------   37.2/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     --------------------------------------   37.3/38.8 MB 1.5 MB/s eta 0:00:02\n",
      "     --------------------------------------   37.3/38.8 MB 1.4 MB/s eta 0:00:01\n",
      "     --------------------------------------   37.4/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     --------------------------------------   37.4/38.8 MB 1.4 MB/s eta 0:00:01\n",
      "     --------------------------------------   37.4/38.8 MB 1.4 MB/s eta 0:00:01\n",
      "     --------------------------------------   37.5/38.8 MB 1.4 MB/s eta 0:00:01\n",
      "     --------------------------------------   37.6/38.8 MB 1.4 MB/s eta 0:00:01\n",
      "     --------------------------------------   37.6/38.8 MB 1.4 MB/s eta 0:00:01\n",
      "     --------------------------------------   37.8/38.8 MB 1.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  37.9/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.0/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.1/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.1/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.2/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.3/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.4/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.4/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  38.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------  38.6/38.8 MB 986.0 kB/s eta 0:00:01\n",
      "     -------------------------------------  38.6/38.8 MB 983.1 kB/s eta 0:00:01\n",
      "     -------------------------------------  38.6/38.8 MB 983.0 kB/s eta 0:00:01\n",
      "     -------------------------------------  38.7/38.8 MB 980.1 kB/s eta 0:00:01\n",
      "     -------------------------------------  38.7/38.8 MB 980.0 kB/s eta 0:00:01\n",
      "     -------------------------------------  38.7/38.8 MB 978.6 kB/s eta 0:00:01\n",
      "     -------------------------------------  38.7/38.8 MB 978.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 38.8/38.8 MB 969.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.3)\n",
      "Installing collected packages: opencv-python-headless\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\91962\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\cv2\\\\cv2.pyd'\n",
      "Check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade opencv-python opencv-python-headless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c360c6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in c:\\users\\91962\\appdata\\roaming\\python\\python311\\site-packages (4.10.0.84)\n",
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.3)\n",
      "Installing collected packages: opencv-python-headless\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\91962\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\cv2\\\\cv2.pyd'\n",
      "Check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade opencv-python opencv-python-headless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12537f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipykernel in c:\\programdata\\anaconda3\\lib\\site-packages (6.19.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel) (0.1.2)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel) (1.5.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel) (8.12.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel) (8.1.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel) (1.5.6)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel) (23.0)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel) (25.1.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel) (6.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel) (5.7.1)\n",
      "Requirement already satisfied: backcall in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel) (5.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-client>=6.1.12->ipykernel) (2.5.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-client>=6.1.12->ipykernel) (305.1)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
      "Requirement already satisfied: executing in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipykernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5f79655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec myenv in C:\\Users\\91962\\AppData\\Roaming\\jupyter\\kernels\\myenv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This version of python seems to be incorrectly compiled\n",
      "(internal generated filenames are not absolute).\n",
      "This may make the debugger miss breakpoints.\n",
      "Related bug: http://bugs.python.org/issue1666807\n"
     ]
    }
   ],
   "source": [
    "!python -m ipykernel install --user --name=myenv --display-name \"Python (myenv)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9feafc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tracker():\n",
    "    return cv2.TrackerKCF_create()  # KCF tracker for OpenCV 3.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83d42156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tracker():\n",
    "    if cv2.__version__.startswith('4'):\n",
    "        return cv2.TrackerCSRT_create()  # Use CSRT for OpenCV 4.x\n",
    "    elif cv2.__version__.startswith('3'):\n",
    "        return cv2.TrackerKCF_create()  # Use KCF for OpenCV 3.x\n",
    "    else:\n",
    "        raise Exception(\"Unsupported OpenCV version. Please update or install a compatible version.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f804cb9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'MultiTracker_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize object tracker\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 19\u001b[0m     tracker \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mMultiTracker_create()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     tracker \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mMultiTracker_create()  \u001b[38;5;66;03m# This works for OpenCV 3.x as well\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'MultiTracker_create'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "# Initialize object tracker\n",
    "if cv2.__version__.startswith('4'):\n",
    "    tracker = cv2.MultiTracker_create()\n",
    "else:\n",
    "    tracker = cv2.MultiTracker_create()  # This works for OpenCV 3.x as well\n",
    "\n",
    "# Dictionary to store object states and movement history\n",
    "object_movement = defaultdict(lambda: {'last_position': None, 'frame_counter': 0})\n",
    "\n",
    "def classify_animal_simulated(cropped_image):\n",
    "    # Simulate classification (replace with actual model or logic)\n",
    "    return np.random.choice(['Healthy', 'Injured'])\n",
    "\n",
    "def classify_object(x, y, w, h):\n",
    "    # This function will simulate classification of an object\n",
    "    # Replace this with real classification logic\n",
    "    return classify_animal_simulated(None)\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:  # Confidence threshold\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Update the tracker with detected objects\n",
    "    new_boxes = []\n",
    "    for i in indexes.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        new_boxes.append((x, y, w, h))\n",
    "    \n",
    "    # If tracking is initialized, update trackers\n",
    "    if len(tracker.getObjects()) > 0:\n",
    "        success, tracked_boxes = tracker.update(frame)\n",
    "        tracked_boxes = [box for box in tracked_boxes if success]  # Filter out unsuccessful trackers\n",
    "        trackers_to_update = defaultdict(list)\n",
    "        for i, box in enumerate(tracked_boxes):\n",
    "            x, y, w, h = list(map(int, box))\n",
    "            current_position = (x, y, w, h)\n",
    "            \n",
    "            # Movement analysis\n",
    "            if object_movement[i]['last_position'] is not None:\n",
    "                last_position = object_movement[i]['last_position']\n",
    "                if abs(current_position[0] - last_position[0]) < 10 and abs(current_position[1] - last_position[1]) < 10:\n",
    "                    object_movement[i]['frame_counter'] += 1\n",
    "                else:\n",
    "                    object_movement[i]['frame_counter'] = 0\n",
    "                object_movement[i]['last_position'] = current_position\n",
    "            else:\n",
    "                object_movement[i]['last_position'] = current_position\n",
    "\n",
    "            if object_movement[i]['frame_counter'] > 50:  # Adjust the threshold based on your video\n",
    "                final_state = \"Dead\"\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            else:\n",
    "                final_state = classify_object(x, y, w, h)\n",
    "                if final_state == \"Dead\":\n",
    "                    color = (0, 0, 255)  # Red for dead\n",
    "                    alert_message = \"Emergency Alert: Animal Dead\"\n",
    "                elif final_state == \"Injured\":\n",
    "                    color = (0, 255, 255)  # Yellow for injured\n",
    "                    alert_message = \"Alert: Animal Injured\"\n",
    "                else:\n",
    "                    color = (0, 255, 0)  # Green for healthy\n",
    "                    alert_message = \"Animal Healthy\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{classes[class_ids[i]]} ({final_state})\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            print(alert_message)\n",
    "\n",
    "    # Initialize new trackers for detected objects\n",
    "    if len(new_boxes) > 0:\n",
    "        for box in new_boxes:\n",
    "            tracker.add(create_tracker(), frame, tuple(box))\n",
    "            object_movement[len(tracker.getObjects()) - 1] = {'last_position': None, 'frame_counter': 0}\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "def create_tracker():\n",
    "    # Create a tracker based on OpenCV version\n",
    "    if cv2.__version__.startswith('4'):\n",
    "        return cv2.TrackerCSRT_create()  # Use CSRT tracker for OpenCV 4.x\n",
    "    elif cv2.__version__.startswith('3'):\n",
    "        return cv2.TrackerKCF_create()  # Use KCF tracker for OpenCV 3.x\n",
    "    else:\n",
    "        raise Exception(\"Unsupported OpenCV version. Please update or install a compatible version.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7760232c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'TrackerCSRT_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indexes\u001b[38;5;241m.\u001b[39mflatten():\n\u001b[0;32m     76\u001b[0m     x, y, w, h \u001b[38;5;241m=\u001b[39m boxes[i]\n\u001b[1;32m---> 77\u001b[0m     tracker \u001b[38;5;241m=\u001b[39m create_tracker()\n\u001b[0;32m     78\u001b[0m     tracker\u001b[38;5;241m.\u001b[39minit(frame, (x, y, w, h))\n\u001b[0;32m     79\u001b[0m     new_trackers\u001b[38;5;241m.\u001b[39mappend(tracker)\n",
      "Cell \u001b[1;32mIn[35], line 7\u001b[0m, in \u001b[0;36mcreate_tracker\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_tracker\u001b[39m():\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Use CSRT tracker which is more robust and compatible with most OpenCV versions\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mTrackerCSRT_create()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'TrackerCSRT_create'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_tracker():\n",
    "    # Use CSRT tracker which is more robust and compatible with most OpenCV versions\n",
    "    return cv2.TrackerCSRT_create()\n",
    "\n",
    "def classify_animal_simulated(cropped_image):\n",
    "    # Simulate classification (replace with actual model or logic)\n",
    "    return np.random.choice(['Healthy', 'Injured'])\n",
    "\n",
    "def classify_object(x, y, w, h):\n",
    "    # This function will simulate classification of an object\n",
    "    # Replace this with real classification logic\n",
    "    return classify_animal_simulated(None)\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "# Initialize trackers and state management\n",
    "trackers = []\n",
    "object_movement = defaultdict(lambda: {'last_position': None, 'frame_counter': 0, 'tracker_index': -1})\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:  # Confidence threshold\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Update the trackers with detected objects\n",
    "    new_trackers = []\n",
    "    for i in indexes.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        tracker = create_tracker()\n",
    "        tracker.init(frame, (x, y, w, h))\n",
    "        new_trackers.append(tracker)\n",
    "        object_movement[len(trackers)] = {'last_position': None, 'frame_counter': 0, 'tracker_index': len(trackers)}\n",
    "    \n",
    "    trackers.extend(new_trackers)\n",
    "\n",
    "    # Track objects and classify their state\n",
    "    for i, tracker in enumerate(trackers):\n",
    "        success, box = tracker.update(frame)\n",
    "        if success:\n",
    "            x, y, w, h = list(map(int, box))\n",
    "            current_position = (x, y, w, h)\n",
    "\n",
    "            # Movement analysis\n",
    "            if object_movement[i]['last_position'] is not None:\n",
    "                last_position = object_movement[i]['last_position']\n",
    "                if abs(current_position[0] - last_position[0]) < 10 and abs(current_position[1] - last_position[1]) < 10:\n",
    "                    object_movement[i]['frame_counter'] += 1\n",
    "                else:\n",
    "                    object_movement[i]['frame_counter'] = 0\n",
    "                object_movement[i]['last_position'] = current_position\n",
    "            else:\n",
    "                object_movement[i]['last_position'] = current_position\n",
    "\n",
    "            # Classify based on movement\n",
    "            if object_movement[i]['frame_counter'] > 50:  # Adjust the threshold based on your video\n",
    "                final_state = \"Dead\"\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            else:\n",
    "                final_state = classify_object(x, y, w, h)\n",
    "                if final_state == \"Dead\":\n",
    "                    color = (0, 0, 255)  # Red for dead\n",
    "                    alert_message = \"Emergency Alert: Animal Dead\"\n",
    "                elif final_state == \"Injured\":\n",
    "                    color = (0, 255, 255)  # Yellow for injured\n",
    "                    alert_message = \"Alert: Animal Injured\"\n",
    "                else:\n",
    "                    color = (0, 255, 0)  # Green for healthy\n",
    "                    alert_message = \"Animal Healthy\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{classes[class_ids[i]]} ({final_state})\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            print(alert_message)\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "354081cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'TrackerKCF_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 9\u001b[0m, in \u001b[0;36mcreate_tracker\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mTrackerCSRT_create()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'TrackerCSRT_create'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 84\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indexes\u001b[38;5;241m.\u001b[39mflatten():\n\u001b[0;32m     83\u001b[0m     x, y, w, h \u001b[38;5;241m=\u001b[39m boxes[i]\n\u001b[1;32m---> 84\u001b[0m     tracker \u001b[38;5;241m=\u001b[39m create_tracker()\n\u001b[0;32m     85\u001b[0m     tracker\u001b[38;5;241m.\u001b[39minit(frame, (x, y, w, h))\n\u001b[0;32m     86\u001b[0m     new_trackers\u001b[38;5;241m.\u001b[39mappend(tracker)\n",
      "Cell \u001b[1;32mIn[36], line 11\u001b[0m, in \u001b[0;36mcreate_tracker\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mTrackerCSRT_create()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mTrackerKCF_create()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Use trackers available in OpenCV 3.x\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mTrackerKCF_create()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'TrackerKCF_create'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_tracker():\n",
    "    if cv2.__version__.startswith('4'):\n",
    "        # Try different trackers available in OpenCV 4.x\n",
    "        try:\n",
    "            return cv2.TrackerCSRT_create()\n",
    "        except AttributeError:\n",
    "            return cv2.TrackerKCF_create()\n",
    "    else:\n",
    "        # Use trackers available in OpenCV 3.x\n",
    "        return cv2.TrackerKCF_create()\n",
    "\n",
    "def classify_animal_simulated(cropped_image):\n",
    "    # Simulate classification (replace with actual model or logic)\n",
    "    return np.random.choice(['Healthy', 'Injured'])\n",
    "\n",
    "def classify_object(x, y, w, h):\n",
    "    # This function will simulate classification of an object\n",
    "    # Replace this with real classification logic\n",
    "    return classify_animal_simulated(None)\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "# Initialize trackers and state management\n",
    "trackers = []\n",
    "object_movement = defaultdict(lambda: {'last_position': None, 'frame_counter': 0, 'tracker_index': -1})\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:  # Confidence threshold\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Update the trackers with detected objects\n",
    "    new_trackers = []\n",
    "    for i in indexes.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        tracker = create_tracker()\n",
    "        tracker.init(frame, (x, y, w, h))\n",
    "        new_trackers.append(tracker)\n",
    "        object_movement[len(trackers)] = {'last_position': None, 'frame_counter': 0, 'tracker_index': len(trackers)}\n",
    "    \n",
    "    trackers.extend(new_trackers)\n",
    "\n",
    "    # Track objects and classify their state\n",
    "    for i, tracker in enumerate(trackers):\n",
    "        success, box = tracker.update(frame)\n",
    "        if success:\n",
    "            x, y, w, h = list(map(int, box))\n",
    "            current_position = (x, y, w, h)\n",
    "\n",
    "            # Movement analysis\n",
    "            if object_movement[i]['last_position'] is not None:\n",
    "                last_position = object_movement[i]['last_position']\n",
    "                if abs(current_position[0] - last_position[0]) < 10 and abs(current_position[1] - last_position[1]) < 10:\n",
    "                    object_movement[i]['frame_counter'] += 1\n",
    "                else:\n",
    "                    object_movement[i]['frame_counter'] = 0\n",
    "                object_movement[i]['last_position'] = current_position\n",
    "            else:\n",
    "                object_movement[i]['last_position'] = current_position\n",
    "\n",
    "            # Classify based on movement\n",
    "            if object_movement[i]['frame_counter'] > 50:  # Adjust the threshold based on your video\n",
    "                final_state = \"Dead\"\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            else:\n",
    "                final_state = classify_object(x, y, w, h)\n",
    "                if final_state == \"Dead\":\n",
    "                    color = (0, 0, 255)  # Red for dead\n",
    "                    alert_message = \"Emergency Alert: Animal Dead\"\n",
    "                elif final_state == \"Injured\":\n",
    "                    color = (0, 255, 255)  # Yellow for injured\n",
    "                    alert_message = \"Alert: Animal Injured\"\n",
    "                else:\n",
    "                    color = (0, 255, 0)  # Green for healthy\n",
    "                    alert_message = \"Animal Healthy\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{classes[class_ids[i]]} ({final_state})\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            print(alert_message)\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c2c2085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: opencv-contrib-python\n"
     ]
    }
   ],
   "source": [
    "!pip show opencv-contrib-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55c74219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.10.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "455a1da4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'TrackerKCF_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 21\u001b[0m\n\u001b[0;32m     20\u001b[0m     tracker_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCSRT\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Change to 'KCF' if CSRT is not available\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     tracker \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mTrackerCSRT_create()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'TrackerCSRT_create'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m         tracker_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKCF\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 24\u001b[0m         tracker \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mTrackerKCF_create()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     tracker_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKCF\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'TrackerKCF_create'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "# Initialize tracker\n",
    "if cv2.__version__.startswith('4'):\n",
    "    try:\n",
    "        tracker_type = 'CSRT'  # Change to 'KCF' if CSRT is not available\n",
    "        tracker = cv2.TrackerCSRT_create()\n",
    "    except AttributeError:\n",
    "        tracker_type = 'KCF'\n",
    "        tracker = cv2.TrackerKCF_create()\n",
    "else:\n",
    "    tracker_type = 'KCF'\n",
    "    tracker = cv2.TrackerKCF_create()\n",
    "\n",
    "# Dictionary to store object states and movement history\n",
    "object_movement = defaultdict(lambda: {'last_position': None, 'frame_counter': 0})\n",
    "\n",
    "def classify_animal_simulated(cropped_image):\n",
    "    # Simulate classification (replace with actual model or logic)\n",
    "    return np.random.choice(['Healthy', 'Injured'])\n",
    "\n",
    "def classify_object(x, y, w, h):\n",
    "    # This function will simulate classification of an object\n",
    "    # Replace this with real classification logic\n",
    "    return classify_animal_simulated(None)\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Processing detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:  # Confidence threshold\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Initialize new trackers for detected objects\n",
    "    new_trackers = []\n",
    "    new_boxes = []\n",
    "    if cv2.__version__.startswith('4'):\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            new_boxes.append((x, y, w, h))\n",
    "            new_tracker = cv2.TrackerCSRT_create() if tracker_type == 'CSRT' else cv2.TrackerKCF_create()\n",
    "            new_tracker.init(frame, (x, y, w, h))\n",
    "            new_trackers.append(new_tracker)\n",
    "    else:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            new_boxes.append((x, y, w, h))\n",
    "            new_tracker = cv2.TrackerKCF_create()\n",
    "            new_tracker.init(frame, (x, y, w, h))\n",
    "            new_trackers.append(new_tracker)\n",
    "\n",
    "    trackers = new_trackers\n",
    "\n",
    "    # Track objects and classify their state\n",
    "    if trackers:\n",
    "        for i, tracker in enumerate(trackers):\n",
    "            success, box = tracker.update(frame)\n",
    "            if success:\n",
    "                x, y, w, h = list(map(int, box))\n",
    "                current_position = (x, y, w, h)\n",
    "\n",
    "                # Movement analysis\n",
    "                if object_movement[i]['last_position'] is not None:\n",
    "                    last_position = object_movement[i]['last_position']\n",
    "                    if abs(current_position[0] - last_position[0]) < 10 and abs(current_position[1] - last_position[1]) < 10:\n",
    "                        object_movement[i]['frame_counter'] += 1\n",
    "                    else:\n",
    "                        object_movement[i]['frame_counter'] = 0\n",
    "                    object_movement[i]['last_position'] = current_position\n",
    "                else:\n",
    "                    object_movement[i]['last_position'] = current_position\n",
    "\n",
    "                if object_movement[i]['frame_counter'] > 50:  # Adjust the threshold based on your video\n",
    "                    final_state = \"Dead\"\n",
    "                    color = (0, 0, 255)  # Red for dead\n",
    "                    alert_message = \"Emergency Alert: Animal Dead\"\n",
    "                else:\n",
    "                    final_state = classify_object(x, y, w, h)\n",
    "                    if final_state == \"Dead\":\n",
    "                        color = (0, 0, 255)  # Red for dead\n",
    "                        alert_message = \"Emergency Alert: Animal Dead\"\n",
    "                    elif final_state == \"Injured\":\n",
    "                        color = (0, 255, 255)  # Yellow for injured\n",
    "                        alert_message = \"Alert: Animal Injured\"\n",
    "                    else:\n",
    "                        color = (0, 255, 0)  # Green for healthy\n",
    "                        alert_message = \"Animal Healthy\"\n",
    "\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(frame, f\"{classes[class_ids[i]]} ({final_state})\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "                print(alert_message)\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb47b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tracker():\n",
    "    return cv2.TrackerCSRT_create()  # CSRT tracker is recommended for OpenCV 4.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4416c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Trackers: ['Tracker', 'TrackerDaSiamRPN', 'TrackerDaSiamRPN_Params', 'TrackerDaSiamRPN_create', 'TrackerGOTURN', 'TrackerGOTURN_Params', 'TrackerGOTURN_create', 'TrackerMIL', 'TrackerMIL_Params', 'TrackerMIL_create', 'TrackerNano', 'TrackerNano_Params', 'TrackerNano_create', 'TrackerVit', 'TrackerVit_Params', 'TrackerVit_create', 'gapi_ot_ObjectTrackerParams']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# List available tracker classes\n",
    "available_trackers = [attr for attr in dir(cv2) if 'Tracker' in attr]\n",
    "print(\"Available Trackers:\", available_trackers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49a684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "# Initialize object tracker\n",
    "tracker = cv2.TrackerMIL_create()\n",
    "\n",
    "# Dictionary to store object states and movement history\n",
    "object_movement = defaultdict(lambda: {'last_position': None, 'frame_counter': 0})\n",
    "\n",
    "# Define a function to classify the state of the animal\n",
    "def classify_animal(cropped_image):\n",
    "    # Simulate classification based on image content\n",
    "    # Replace this with actual image analysis logic\n",
    "    if np.random.random() > 0.5:\n",
    "        return 'Injured'\n",
    "    else:\n",
    "        return 'Healthy'\n",
    "\n",
    "def process_frame(frame, trackers):\n",
    "    for i, tracker in enumerate(trackers):\n",
    "        success, box = tracker.update(frame)\n",
    "        if success:\n",
    "            x, y, w, h = list(map(int, box))\n",
    "            # Movement analysis\n",
    "            if object_movement[i]['last_position'] is not None:\n",
    "                last_position = object_movement[i]['last_position']\n",
    "                if abs(x - last_position[0]) < 10 and abs(y - last_position[1]) < 10:\n",
    "                    object_movement[i]['frame_counter'] += 1\n",
    "                else:\n",
    "                    object_movement[i]['frame_counter'] = 0\n",
    "                object_movement[i]['last_position'] = (x, y)\n",
    "            else:\n",
    "                object_movement[i]['last_position'] = (x, y)\n",
    "            \n",
    "            # Classification\n",
    "            if object_movement[i]['frame_counter'] > 50:  # Adjust the threshold based on your video\n",
    "                final_state = \"Dead\"\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            else:\n",
    "                final_state = classify_animal(None)\n",
    "                if final_state == \"Injured\":\n",
    "                    color = (0, 255, 255)  # Yellow for injured\n",
    "                    alert_message = \"Alert: Animal Injured\"\n",
    "                else:\n",
    "                    color = (0, 255, 0)  # Green for healthy\n",
    "                    alert_message = \"Animal Healthy\"\n",
    "                \n",
    "            # Draw rectangle and text on frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{final_state}\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            print(alert_message)\n",
    "\n",
    "def detect_objects(frame, net, output_layers):\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    return indexes, boxes\n",
    "\n",
    "# Main loop\n",
    "trackers = []\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    indexes, boxes = detect_objects(frame, net, output_layers)\n",
    "    \n",
    "    if len(trackers) > 0:\n",
    "        process_frame(frame, trackers)\n",
    "    \n",
    "    # Initialize new trackers for detected objects\n",
    "    if len(boxes) > 0:\n",
    "        if len(trackers) == 0:\n",
    "            for i in indexes.flatten():\n",
    "                x, y, w, h = boxes[i]\n",
    "                tracker = cv2.TrackerMIL_create()\n",
    "                tracker.init(frame, (x, y, w, h))\n",
    "                trackers.append(tracker)\n",
    "                object_movement[len(trackers) - 1] = {'last_position': None, 'frame_counter': 0}\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#bad\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "# Initialize object tracker\n",
    "tracker = cv2.TrackerMIL_create()\n",
    "\n",
    "# Dictionary to store object states and movement history\n",
    "object_movement = defaultdict(lambda: {'last_position': None, 'frame_counter': 0})\n",
    "\n",
    "# Define a function to classify the state of the animal\n",
    "def classify_animal(cropped_image):\n",
    "    # Simulate classification based on image content\n",
    "    # Replace this with actual image analysis logic\n",
    "    if np.random.random() > 0.5:\n",
    "        return 'Injured'\n",
    "    else:\n",
    "        return 'Healthy'\n",
    "\n",
    "def process_frame(frame, trackers):\n",
    "    for i, tracker in enumerate(trackers):\n",
    "        success, box = tracker.update(frame)\n",
    "        if success:\n",
    "            x, y, w, h = list(map(int, box))\n",
    "            # Movement analysis\n",
    "            if object_movement[i]['last_position'] is not None:\n",
    "                last_position = object_movement[i]['last_position']\n",
    "                if abs(x - last_position[0]) < 10 and abs(y - last_position[1]) < 10:\n",
    "                    object_movement[i]['frame_counter'] += 1\n",
    "                else:\n",
    "                    object_movement[i]['frame_counter'] = 0\n",
    "                object_movement[i]['last_position'] = (x, y)\n",
    "            else:\n",
    "                object_movement[i]['last_position'] = (x, y)\n",
    "            \n",
    "            # Classification based on inactivity and appearance\n",
    "            if object_movement[i]['frame_counter'] > 50:  # Threshold for inactivity\n",
    "                final_state = \"Dead\"\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            else:\n",
    "                final_state = classify_animal(None)\n",
    "                if final_state == \"Injured\":\n",
    "                    color = (0, 255, 255)  # Yellow for injured\n",
    "                    alert_message = \"Alert: Animal Injured\"\n",
    "                else:\n",
    "                    color = (0, 255, 0)  # Green for healthy\n",
    "                    alert_message = \"Animal Healthy\"\n",
    "                \n",
    "            # Draw rectangle and text on frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{final_state}\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            print(alert_message)\n",
    "\n",
    "def detect_objects(frame, net, output_layers):\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    return indexes, boxes\n",
    "\n",
    "# Main loop\n",
    "trackers = []\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    indexes, boxes = detect_objects(frame, net, output_layers)\n",
    "    \n",
    "    if len(trackers) > 0:\n",
    "        process_frame(frame, trackers)\n",
    "    \n",
    "    # Initialize new trackers for detected objects\n",
    "    if len(boxes) > 0:\n",
    "        if len(trackers) == 0:\n",
    "            for i in indexes.flatten():\n",
    "                x, y, w, h = boxes[i]\n",
    "                tracker = cv2.TrackerMIL_create()\n",
    "                tracker.init(frame, (x, y, w, h))\n",
    "                trackers.append(tracker)\n",
    "                object_movement[len(trackers) - 1] = {'last_position': None, 'frame_counter': 0}\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598dce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "# Initialize object tracker\n",
    "tracker = cv2.TrackerMIL_create()\n",
    "\n",
    "# Dictionary to store object states and movement history\n",
    "object_movement = defaultdict(lambda: {'last_position': None, 'frame_counter': 0})\n",
    "\n",
    "# Define a function to classify the state of the animal\n",
    "def classify_animal(cropped_image):\n",
    "    # Simulate classification based on image content\n",
    "    # Replace this with actual image analysis logic\n",
    "    if np.random.random() > 0.5:\n",
    "        return 'Injured'\n",
    "    else:\n",
    "        return 'Healthy'\n",
    "\n",
    "def process_frame(frame, trackers):\n",
    "    for i, (tracker, box_id) in enumerate(trackers):\n",
    "        success, box = tracker.update(frame)\n",
    "        if success:\n",
    "            x, y, w, h = list(map(int, box))\n",
    "            # Movement analysis\n",
    "            if object_movement[box_id]['last_position'] is not None:\n",
    "                last_position = object_movement[box_id]['last_position']\n",
    "                if abs(x - last_position[0]) < 10 and abs(y - last_position[1]) < 10:\n",
    "                    object_movement[box_id]['frame_counter'] += 1\n",
    "                else:\n",
    "                    object_movement[box_id]['frame_counter'] = 0\n",
    "                object_movement[box_id]['last_position'] = (x, y)\n",
    "            else:\n",
    "                object_movement[box_id]['last_position'] = (x, y)\n",
    "            \n",
    "            # Classification based on inactivity and appearance\n",
    "            if object_movement[box_id]['frame_counter'] > 50:  # Threshold for inactivity\n",
    "                final_state = \"Dead\"\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            else:\n",
    "                final_state = classify_animal(None)\n",
    "                if final_state == \"Injured\":\n",
    "                    color = (0, 255, 255)  # Yellow for injured\n",
    "                    alert_message = \"Alert: Animal Injured\"\n",
    "                else:\n",
    "                    color = (0, 255, 0)  # Green for healthy\n",
    "                    alert_message = \"Animal Healthy\"\n",
    "                \n",
    "            # Draw rectangle and text on frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{final_state}\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            print(alert_message)\n",
    "\n",
    "def detect_objects(frame, net, output_layers):\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    return indexes, boxes\n",
    "\n",
    "\n",
    "trackers = []\n",
    "box_ids = []\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    indexes, boxes = detect_objects(frame, net, output_layers)\n",
    "    \n",
    "    if len(trackers) > 0:\n",
    "        process_frame(frame, zip(trackers, box_ids))\n",
    "    \n",
    "    \n",
    "    if len(boxes) > 0:\n",
    "        \n",
    "        new_trackers = []\n",
    "        new_box_ids = []\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            new_tracker = cv2.TrackerMIL_create()\n",
    "            new_tracker.init(frame, (x, y, w, h))\n",
    "            new_trackers.append(new_tracker)\n",
    "            new_box_ids.append(len(trackers) + len(new_box_ids))\n",
    "            object_movement[len(trackers) + len(new_box_ids) - 1] = {'last_position': None, 'frame_counter': 0}\n",
    "\n",
    "        trackers.extend(new_trackers)\n",
    "        box_ids.extend(new_box_ids)\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7718e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "# Dictionary to store trackers and their states\n",
    "trackers = {}\n",
    "object_movement = defaultdict(lambda: {'last_position': None, 'frame_counter': 0})\n",
    "\n",
    "# Define a function to classify the state of the animal\n",
    "def classify_animal(cropped_image):\n",
    "    # Simulate classification based on image content\n",
    "    # Replace this with actual image analysis logic\n",
    "    if np.random.random() > 0.5:\n",
    "        return 'Injured'\n",
    "    else:\n",
    "        return 'Healthy'\n",
    "\n",
    "def process_frame(frame):\n",
    "    # Update trackers and remove those that are not valid\n",
    "    valid_trackers = {}\n",
    "    for box_id, tracker in trackers.items():\n",
    "        success, box = tracker.update(frame)\n",
    "        if success:\n",
    "            x, y, w, h = list(map(int, box))\n",
    "            if object_movement[box_id]['last_position'] is not None:\n",
    "                last_position = object_movement[box_id]['last_position']\n",
    "                if abs(x - last_position[0]) < 10 and abs(y - last_position[1]) < 10:\n",
    "                    object_movement[box_id]['frame_counter'] += 1\n",
    "                else:\n",
    "                    object_movement[box_id]['frame_counter'] = 0\n",
    "                object_movement[box_id]['last_position'] = (x, y)\n",
    "            else:\n",
    "                object_movement[box_id]['last_position'] = (x, y)\n",
    "            \n",
    "         \n",
    "            if object_movement[box_id]['frame_counter'] > 50: \n",
    "                final_state = \"Dead\"\n",
    "                color = (0, 0, 255)  \n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            else:\n",
    "                final_state = classify_animal(None)\n",
    "                if final_state == \"Injured\":\n",
    "                    color = (0, 255, 255) \n",
    "                    alert_message = \"Alert: Animal Injured\"\n",
    "                else:\n",
    "                    color = (0, 255, 0) \n",
    "                    alert_message = \"Animal Healthy\"\n",
    "                \n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{final_state}\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            print(alert_message)\n",
    "\n",
    "            # Keep track of valid trackers\n",
    "            valid_trackers[box_id] = tracker\n",
    "    \n",
    "   \n",
    "    trackers.clear()\n",
    "    trackers.update(valid_trackers)\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    return indexes, boxes\n",
    "\n",
    "# Main loop\n",
    "object_id = 0\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    indexes, boxes = detect_objects(frame)\n",
    "    \n",
    "    if len(trackers) > 0:\n",
    "        process_frame(frame)\n",
    "    \n",
    "    # Initialize new trackers for detected objects\n",
    "    new_trackers = {}\n",
    "    if len(boxes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            # Only initialize new trackers if they don't already exist\n",
    "            is_new = True\n",
    "            for box_id in trackers.keys():\n",
    "                success, box = trackers[box_id].update(frame)\n",
    "                if success and abs(x - box[0]) < 10 and abs(y - box[1]) < 10:\n",
    "                    is_new = False\n",
    "                    break\n",
    "            \n",
    "            if is_new:\n",
    "                tracker = cv2.TrackerMIL_create()\n",
    "                tracker.init(frame, (x, y, w, h))\n",
    "                new_trackers[object_id] = tracker\n",
    "                object_movement[object_id] = {'last_position': None, 'frame_counter': 0}\n",
    "                object_id += 1\n",
    "\n",
    "    trackers.update(new_trackers)\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be18d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Open the pre-recorded video\n",
    "video_capture = cv2.VideoCapture('hackx.mp4')\n",
    "\n",
    "# Dictionary to store trackers and their states\n",
    "trackers = {}\n",
    "object_movement = defaultdict(lambda: {'last_position': None, 'frame_counter': 0})\n",
    "\n",
    "# Placeholder for the actual model or classifier\n",
    "def classify_animal(cropped_image):\n",
    "    # This function should be replaced with your actual image analysis/model\n",
    "    # For now, it will simulate classification randomly\n",
    "    injury_probability = np.random.random()  # Simulate a probability\n",
    "    return 'Injured' if injury_probability > 0.5 else 'Healthy'\n",
    "\n",
    "def process_frame(frame):\n",
    "    # Update trackers and remove those that are not valid\n",
    "    valid_trackers = {}\n",
    "    for box_id, tracker in trackers.items():\n",
    "        success, box = tracker.update(frame)\n",
    "        if success:\n",
    "            x, y, w, h = list(map(int, box))\n",
    "            if object_movement[box_id]['last_position'] is not None:\n",
    "                last_position = object_movement[box_id]['last_position']\n",
    "                if abs(x - last_position[0]) < 10 and abs(y - last_position[1]) < 10:\n",
    "                    object_movement[box_id]['frame_counter'] += 1\n",
    "                else:\n",
    "                    object_movement[box_id]['frame_counter'] = 0\n",
    "                object_movement[box_id]['last_position'] = (x, y)\n",
    "            else:\n",
    "                object_movement[box_id]['last_position'] = (x, y)\n",
    "            \n",
    "            # Classification based on inactivity and appearance\n",
    "            if object_movement[box_id]['frame_counter'] > 50:  # Threshold for inactivity\n",
    "                final_state = \"Dead\"\n",
    "                color = (0, 0, 255)  # Red for dead\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            else:\n",
    "                cropped_image = frame[y:y + h, x:x + w]\n",
    "                final_state = classify_animal(cropped_image)\n",
    "                if final_state == \"Injured\":\n",
    "                    color = (0, 255, 255)  # Yellow for injured\n",
    "                    alert_message = \"Alert: Animal Injured\"\n",
    "                else:\n",
    "                    color = (0, 255, 0)  \n",
    "                    alert_message = \"Animal Healthy\"\n",
    "                \n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{final_state}\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            print(alert_message)\n",
    "\n",
    "          \n",
    "            valid_trackers[box_id] = tracker\n",
    "    \n",
    "    \n",
    "    trackers.clear()\n",
    "    trackers.update(valid_trackers)\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    return indexes, boxes\n",
    "\n",
    "# Main loop\n",
    "object_id = 0\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    indexes, boxes = detect_objects(frame)\n",
    "    \n",
    "    if len(trackers) > 0:\n",
    "        process_frame(frame)\n",
    "    \n",
    "    # Initialize new trackers for detected objects\n",
    "    new_trackers = {}\n",
    "    if len(boxes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            # Only initialize new trackers if they don't already exist\n",
    "            is_new = True\n",
    "            for box_id in trackers.keys():\n",
    "                success, box = trackers[box_id].update(frame)\n",
    "                if success and abs(x - box[0]) < 10 and abs(y - box[1]) < 10:\n",
    "                    is_new = False\n",
    "                    break\n",
    "            \n",
    "            if is_new:\n",
    "                tracker = cv2.TrackerMIL_create()\n",
    "                tracker.init(frame, (x, y, w, h))\n",
    "                new_trackers[object_id] = tracker\n",
    "                object_movement[object_id] = {'last_position': None, 'frame_counter': 0}\n",
    "                object_id += 1\n",
    "\n",
    "    trackers.update(new_trackers)\n",
    "\n",
    "    # Show the frame with detected objects\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e05051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "video_capture = cv2.VideoCapture('istockphoto-184292401-612x612.jpg')\n",
    "\n",
    "trackers = {}\n",
    "object_movement = defaultdict(lambda: {'last_position': None, 'frame_counter': 0})\n",
    "\n",
    "def classify_animal(cropped_image):\n",
    "    injury_probability = np.random.random()\n",
    "    return 'Injured' if injury_probability > 0.5 else 'Healthy'\n",
    "\n",
    "def process_frame(frame):\n",
    "    valid_trackers = {}\n",
    "    for box_id, tracker in trackers.items():\n",
    "        success, box = tracker.update(frame)\n",
    "        if success:\n",
    "            x, y, w, h = list(map(int, box))\n",
    "            if object_movement[box_id]['last_position'] is not None:\n",
    "                last_position = object_movement[box_id]['last_position']\n",
    "                if abs(x - last_position[0]) < 10 and abs(y - last_position[1]) < 10:\n",
    "                    object_movement[box_id]['frame_counter'] += 1\n",
    "                else:\n",
    "                    object_movement[box_id]['frame_counter'] = 0\n",
    "                object_movement[box_id]['last_position'] = (x, y)\n",
    "            else:\n",
    "                object_movement[box_id]['last_position'] = (x, y)\n",
    "\n",
    "            if object_movement[box_id]['frame_counter'] > 20:\n",
    "                final_state = \"Dead\"\n",
    "                color = (0, 0, 255)\n",
    "                alert_message = \"Emergency Alert: Animal Dead\"\n",
    "            else:\n",
    "                cropped_image = frame[y:y + h, x:x + w]\n",
    "                final_state = classify_animal(cropped_image)\n",
    "                if final_state == \"Injured\":\n",
    "                    color = (0, 255, 255)\n",
    "                    alert_message = \"Alert: Animal Injured\"\n",
    "                else:\n",
    "                    color = (0, 255, 0)\n",
    "                    alert_message = \"Animal Healthy\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{final_state}\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "            print(alert_message)\n",
    "\n",
    "            valid_trackers[box_id] = tracker\n",
    "\n",
    "    trackers.clear()\n",
    "    trackers.update(valid_trackers)\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    return indexes, boxes\n",
    "\n",
    "def iou(box1, box2):\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "    xi1, yi1 = max(x1, x2), max(y1, y2)\n",
    "    xi2, yi2 = min(x1 + w1, x2 + w2), min(y1 + h1, y2 + h2)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "    return inter_area / (box1_area + box2_area - inter_area)\n",
    "\n",
    "object_id = 0\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    indexes, boxes = detect_objects(frame)\n",
    "\n",
    "    if len(trackers) > 0:\n",
    "        process_frame(frame)\n",
    "\n",
    "    new_trackers = {}\n",
    "    if len(boxes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            is_new = True\n",
    "            for box_id in list(trackers.keys()):\n",
    "                success, box = trackers[box_id].update(frame)\n",
    "                if success:\n",
    "                    box_x, box_y, box_w, box_h = list(map(int, box))\n",
    "                    if iou([x, y, w, h], [box_x, box_y, box_w, box_h]) > 0.5:\n",
    "                        is_new = False\n",
    "                        break\n",
    "\n",
    "            if is_new:\n",
    "                tracker = cv2.TrackerMIL_create()\n",
    "                tracker.init(frame, (x, y, w, h))\n",
    "                new_trackers[object_id] = tracker\n",
    "                object_movement[object_id] = {'last_position': None, 'frame_counter': 0}\n",
    "                object_id += 1\n",
    "\n",
    "    trackers.update(new_trackers)\n",
    "\n",
    "    cv2.imshow('Video Feed with Detection and Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e691e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert: Animal Injured\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Load image\n",
    "image_path = 'img1.jpg'\n",
    "frame = cv2.imread(image_path)\n",
    "\n",
    "def classify_animal(cropped_image):\n",
    "    injury_probability = np.random.random()\n",
    "    return 'Injured' if injury_probability > 0.5 else 'Healthy'\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    return indexes, boxes\n",
    "\n",
    "def iou(box1, box2):\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "    xi1, yi1 = max(x1, x2), max(y1, y2)\n",
    "    xi2, yi2 = min(x1 + w1, x2 + w2), min(y1 + h1, y2 + h2)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "    return inter_area / (box1_area + box2_area - inter_area)\n",
    "\n",
    "indexes, boxes = detect_objects(frame)\n",
    "\n",
    "for i in indexes.flatten():\n",
    "    x, y, w, h = boxes[i]\n",
    "    cropped_image = frame[y:y + h, x:x + w]\n",
    "    final_state = classify_animal(cropped_image)\n",
    "    color = (0, 255, 255) if final_state == \"Injured\" else (0, 255, 0)\n",
    "    alert_message = \"Alert: Animal Injured\" if final_state == \"Injured\" else \"Animal Healthy\"\n",
    "\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "    cv2.putText(frame, f\"{final_state}\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "    print(alert_message)\n",
    "\n",
    "cv2.imshow('Image with Detection and Classification', frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e6947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Load image\n",
    "image_path = 'img2.jpg'\n",
    "frame = cv2.imread(image_path)\n",
    "\n",
    "def classify_animal(cropped_image):\n",
    "    injury_probability = np.random.random()\n",
    "    return 'Injured' if injury_probability > 0.5 else 'Healthy'\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    return indexes, boxes\n",
    "\n",
    "def iou(box1, box2):\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "    xi1, yi1 = max(x1, x2), max(y1, y2)\n",
    "    xi2, yi2 = min(x1 + w1, x2 + w2), min(y1 + h1, y2 + h2)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "    return inter_area / (box1_area + box2_area - inter_area)\n",
    "\n",
    "indexes, boxes = detect_objects(frame)\n",
    "\n",
    "for i in indexes.flatten():\n",
    "    x, y, w, h = boxes[i]\n",
    "    cropped_image = frame[y:y + h, x:x + w]\n",
    "    final_state = classify_animal(cropped_image)\n",
    "    color = (0, 255, 255) if final_state == \"Injured\" else (0, 255, 0)\n",
    "    alert_message = \"Alert: Animal Injured\" if final_state == \"Injured\" else \"Animal Healthy\"\n",
    "\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "    cv2.putText(frame, f\"{final_state}\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "    print(alert_message)\n",
    "\n",
    "cv2.imshow('Image with Detection and Classification', frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c066c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Load image\n",
    "image_path = 'img3.jpg'\n",
    "frame = cv2.imread(image_path)\n",
    "\n",
    "def classify_animal(cropped_image):\n",
    "    injury_probability = np.random.random()\n",
    "    return 'Injured' if injury_probability > 0.5 else 'Healthy'\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    return indexes, boxes\n",
    "\n",
    "def iou(box1, box2):\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "    xi1, yi1 = max(x1, x2), max(y1, y2)\n",
    "    xi2, yi2 = min(x1 + w1, x2 + w2), min(y1 + h1, y2 + h2)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "    return inter_area / (box1_area + box2_area - inter_area)\n",
    "\n",
    "indexes, boxes = detect_objects(frame)\n",
    "\n",
    "for i in indexes.flatten():\n",
    "    x, y, w, h = boxes[i]\n",
    "    cropped_image = frame[y:y + h, x:x + w]\n",
    "    final_state = classify_animal(cropped_image)\n",
    "    color = (0, 255, 255) if final_state == \"Injured\" else (0, 255, 0)\n",
    "    alert_message = \"Alert: Animal Injured\" if final_state == \"Injured\" else \"Animal Healthy\"\n",
    "\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "    cv2.putText(frame, f\"{final_state}\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "    print(alert_message)\n",
    "\n",
    "cv2.imshow('Image with Detection and Classification', frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40f5a77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert: Animal Injured\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec167003",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Image not loaded. Check the file path: img.jpeg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage not loaded. Check the file path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify_animal\u001b[39m(cropped_image):\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    Force the classification to always return 'Healthy'.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Image not loaded. Check the file path: img.jpeg"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load class names\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Load the image\n",
    "image_path = 'img.jpeg'\n",
    "frame = cv2.imread(image_path)\n",
    "\n",
    "if frame is None:\n",
    "    raise ValueError(f\"Image not loaded. Check the file path: {image_path}\")\n",
    "\n",
    "def classify_animal(cropped_image):\n",
    "    \"\"\"\n",
    "    Force the classification to always return 'Healthy'.\n",
    "    \"\"\"\n",
    "    return 'Dead'\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "           \n",
    "            if confidence > 0.6 and class_id in [16, 17, 18]:  \n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    return indexes, boxes\n",
    "\n",
    "indexes, boxes = detect_objects(frame)\n",
    "\n",
    "for i in indexes.flatten():\n",
    "    x, y, w, h = boxes[i]\n",
    "    cropped_image = frame[y:y + h, x:x + w]\n",
    "    final_state = classify_animal(cropped_image)\n",
    "    color = (0, 255, 0)  \n",
    "    alert_message = \"Animal dead\"\n",
    "\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "    cv2.putText(frame, f\"{final_state}\", (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
    "    print(alert_message)\n",
    "\n",
    "cv2.imshow('Video feed with Detection and Classification', frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3d388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
